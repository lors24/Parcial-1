\documentclass[10pt]{article}
\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}
\usepackage[margin=0.9in]{geometry}

\title{Examen 1}
\author{Lorena Domínguez Ponce}

\begin{document}
\maketitle
<<preambulo, message=FALSE, echo=FALSE, results='hide'>>=
opts_chunk$set(fig.width=4.5,fig.height=3)
set.seed(1)
options(digits = 4)
library(ElemStatLearn)
library(ISLR)
library(kknn)
library(reshape2)
library(ggplot2)
library(dplyr)
library(tidyr)
library(glmnet)
library(ROCR)
@

\setcounter{section}{10}
\section{Ejercicio 11}

\subsection{Preparación de los datos}
Se quiere predecir el salario de un beisbolista en función de varias estadísticas que describen su desempeño. Al ser el salario un dato numérico (cuantitativo), la mejor forma de realizar la predicción es mediante un análisis de regresión. 

<<11.1>>=
rm(list=ls())
data <- Hitters
dim(data) # Tamaño original
data <- data[,sapply(data, is.numeric)]
dim(data) # Tamaño usando sólo las variables numéricas
@

La base \textit{Hitters} contiene información sobre 322 bateadores de las temporadas 1986 y 1987. En la base de datos se tienen 3 variables categóricas. Una opción podría ser crear variables dummy para representar los distintos niveles de esta variable, pero presentarían problemas al momento de regularizar. Por el momento se van a eliminar dichas variables y se utilizarán únicamente las variables numéricas.  Así la base se reduce a 17 de las 20 variables originales. También es importante revisar si hay datos faltantes en la base, ya que esto puede traer complicaciones al momento de realizar la regresión. 

<<11.2>>=
complete <- sapply(1:ncol(data), function(i){
    s <- sum(complete.cases(data[,i]))
})
casos.completos <- data.frame(Variable = names(data), Completos = complete)
casos.completos

proporcion.completos = mean(complete.cases(data)) * 100
proporcion.completos
data.clean = data[complete.cases(data),]
data.clean$id = row.names(data.clean)
row.names(data.clean) <- NULL
@

Se puede observar que que la única variable en la que hay datos faltantes es en \textit{Salary}; la variable que se quiere predecir. Para el análisis, se elimina la información de los beisbolistas en los que no se posee información del salario con lo que la base final consta con el 81$\%$ de los datos originales. \\

Se aparta una muestra de prueba de tamaño 100, el resto (163) será de entrenamiento.

<<11.3>>=
n.test <- 100
indices.test <- sample(data.clean$id, n.test)
data.train <- data.clean %.% filter(!(id %in% indices.test))
data.test <- data.clean %.% filter( id %in% indices.test)
nrow(data.train) # Tamaño de la muestra de entrenamiento
@

A continuación, se estandarizan los datos, lo cual es necesario para Ridge, Lasso y k-Vecinos más cercanos.

<<11.4, message=FALSE>>=
data.train.long <- data.train %>% 
    gather(variable, valor, AtBat:Salary)

media.de <- data.train.long %>%
  group_by(variable) %>%
  summarise(media = mean(valor), de = sd(valor))

data.train.long <- data.train %>%
  gather(variable, valor, AtBat:Salary) %>%
  filter(variable!='Salary') %>%
  left_join(media.de) %>%
  mutate(valor.s = (valor - media) / de)

data.train.s <- data.train.long %>%
  select(id, variable, valor.s) %>%
  spread(variable, valor.s) %>%
  left_join(data.train[, c('id','Salary')])

media.1 <- media.de %>%
  filter(variable!='Salary')

data.test.long <- data.test %>%
  gather(variable, valor, AtBat:Salary) %>%
  filter(variable!='Salary') %>%
  left_join(media.1) %>%
  mutate(valor.s = (valor - media) / de)

data.test.s <- data.test.long %>%
  select(id, variable, valor.s) %>%
  spread(variable, valor.s)
@
Cabe destacar que la estandarización de la base de prueba se realiza con base en la media y la varianza de la muestra de entrenamiento.

\subsection{Regresión Lineal}
<<11.5>>=
train.model.1 <- lm(Salary ~ ., data = data.train.s[, -1] )
train.error.1 = (mean((fitted(train.model.1) - data.train.s$Salary)^2))

test.pred.1 <- predict(train.model.1, newdata = data.test.s)
test.error.1 = (mean((test.pred.1 - data.test$Salary)^2))

res.model.1 <- data.frame(modelo = "Regresion lineal", 
                          err.entrena = train.error.1, err.prueba = test.error.1)
@

\subsection{Regresión Ridge}
<<11.6>>=
train.model.2 <- cv.glmnet(x = as.matrix(data.train.s[, -c(1,18)]), 
                           y = data.train.s$Salary, alpha = 0)
train.pred.2  <- predict(train.model.2, newx = as.matrix(data.train.s[, -c(1,18)]), 
                         s = train.model.2$lambda.1se)
train.error.2 <- mean((train.pred.2 - data.train.s$Salary)^2)

test.pred.2   <- predict(train.model.2, newx = as.matrix(data.test.s[, -1]),
                         s = train.model.2$lambda.1se)
test.error.2  <- mean((test.pred.2 - data.test$Salary)^2)

res.model.2 <- data.frame(modelo = "Regresion Ridge", 
                          err.entrena = train.error.2, err.prueba = test.error.2)
@

\subsection{Regresión Lasso}
<<11.7>>=
train.model.3 <- cv.glmnet(x = as.matrix(data.train.s[, -c(1,18)]), 
                           y = data.train.s$Salary, alpha = 1)
train.pred.3  <- predict(train.model.3, newx = as.matrix(data.train.s[, -c(1,18)]),
                         s = train.model.3$lambda.1se)
train.error.3 <- mean((train.pred.3 - data.train.s$Salary)^2)

test.pred.3   <- predict(train.model.3, newx = as.matrix(data.test.s[, -1]),
                         s = train.model.3$lambda.1se)
test.error.3  <- mean((test.pred.3 - data.test$Salary)^2)

res.model.3 <- data.frame(modelo = "Regresion Lasso", 
                          err.entrena = train.error.3, err.prueba = test.error.3)
@

\subsection{k Vecinos más cercanos}
<<11.8>>=
train.model.4 <- kknn(Salary ~., k = 1, train = data.train.s[, -1],
                      test = data.train.s[, -1], kernel = "rectangular")
train.pred.4  <- fitted(train.model.4)
train.error.4 <- mean((train.pred.4 - data.train.s$Salary)^2)

test.model.4  <- kknn(Salary ~., k = 1, train = data.train.s[, -1],
                      test = data.test.s[, -1], kernel = "rectangular")
test.pred.4   <- predict(train.model.4, newx = as.matrix(data.test.s[, -1]))
test.error.4  <- mean((test.pred.4 - data.test$Salary)^2)

res.model.4 <- data.frame(modelo = "1 vecino mas cercano", 
                          err.entrena = train.error.4, err.prueba = test.error.4)
@

\subsection{Resultados}
A continuación se muestra una tabla con los errores de entrenamiento y de prueba para cada caso. Como se había visto antes, 1 vecino más cercano tiene un error de entrenamiento de 0, sin embargo, lo que realmente nos interesa es el error de prueba. Para este conjunto de datos se obtuvo que el error de prueba es menor para Regresión Ridge. De los resultados, se puede destacar que la regularización de las regresiones Ridge y Lasso ayudó a disminuir el sobreajuste a la base de entrenamiento. Cabe señalar que el coeficiente de penalización que se usó, fue seleccionado usando validación cruzada y eligiendo el modelo más simple a 1 desviación estándar del mínimo.
<<11.9>>=
res.tbl <- rbind(res.model.1, res.model.2, res.model.3, res.model.4)
res.tbl
@

\section{Ejercicio 12}

La base de datos contiene información sobre 462 A retrospective sample of males in a heart-disease high-risk region of the Western Cape, South Africa.
Se consideran los datos de \textit{SAheart} donde se quiere predecir enfermedad del corazón en términos de otras varibles como consumo de tabaco, obesidad, edad, etc. 

<<12.1>>=

rm(list=ls())

data <- SAheart
dim(data)
data$famhist <- as.numeric(data$famhist)-1

@

La base de datos tiene información de 462 pacientes. Primero recodificamos la variables $\textit{famhist}$ para poder utilizarla en la regresión. La variable vale 1 cuando si había historial de enfermedades del corazón en la familia y 0 en otro caso. 


<<12.2>>=
# Recodificamos la variable famhist
# 1 significa presencia de enfermedad del corazon en la familia
# Tenemos datos de 462 pacientes 

N <- 300

indices <- sample(1:nrow(data), N)
data.train <- data[indices, ]
data.test <- data[-indices, ]


x.train <- data.train[, -10]
x.test <- data.test[, -10]
y.train <- data.train[, 'chd']
y.test <- data.test[, 'chd']
@

Tenemos una tasa de error base de:

\subsection{Regresión Ridge}
<<12.3>>=
tasa.base <- 1- max(prop.table(table(data.test$chd)))
mod.ridge <- glmnet(as.matrix(x.train),
                  as.numeric(y.train), 
                  family="binomial", alpha = 0)

mod.ridge.cv <- cv.glmnet(as.matrix(x.train) , 
    as.numeric(y.train), 
    family="binomial", alpha = 0, lambda.min.ratio=1e-4)

pos.lambda <- which(
  abs(mod.ridge.cv$lambda-mod.ridge.cv$lambda.min) < 0.00001)

betas.mod <- mod.ridge$beta[,pos.lambda] 

tab.1 <- data.frame(nombre = names(betas.mod)[betas.mod != 0], 
                    coef = round(betas.mod[betas.mod!=0],3))
tab.1

out.pred.test <- predict(mod.ridge, newx=as.matrix(x.test), type="response")
preds.1 <- out.pred.test[,pos.lambda]
final.pred <- preds.1 > 0.5
@

Tabla de contingencia

<<12.4>>=
tab.final.ridge <- table(final.pred, y.test)

vn <- tab.final.ridge[1,1]
fn <- tab.final.ridge[1,2]
fp <- tab.final.ridge[2,1]
vp <- tab.final.ridge[2,2]

error.prueba.ridge <- (fp + fn ) / length(y.test)
kappa.ridge <- (tasa.base - error.prueba.ridge)/tasa.base

esp.ridge <-vn / (vn + fp)
sen.ridge <- vp / (fn + vp)
pred.ridge <- prediction(preds.1, y.test == 1)
perf.ridge <- performance(pred.ridge, 'sens','fpr')
@

\subsection{Lasso}
<<12.5>>= 
mod.lasso <- glmnet(as.matrix(x.train),
                  as.numeric(y.train), 
                  family="binomial", alpha = 1)
mod.lasso.cv <- cv.glmnet(as.matrix(x.train) , 
    as.numeric(y.train), 
    family = "binomial", alpha = 1, lambda.min.ratio=1e-4)

pos.lambda <- which(
  abs(mod.lasso.cv$lambda-mod.lasso.cv$lambda.min) < 0.00001)

betas.mod <- mod.lasso$beta[,pos.lambda] 
out.pred.test <- predict(mod.lasso, newx=as.matrix(x.test), type="response")
preds.1 <- out.pred.test[,pos.lambda]

final.pred <- preds.1 > 0.5
tab.final.lasso <- table(final.pred, y.test)

error.prueba.lasso <- (tab.final.lasso[1,2]+tab.final.lasso[2,1])/length(y.test)
kappa.lasso <- (tasa.base - error.prueba.lasso )/tasa.base
@
Tabla de contingencia

<<12.6>>=
tab.final.lasso <- table(final.pred, y.test)

vn <- tab.final.lasso[1,1]
fn <- tab.final.lasso[1,2]
fp <- tab.final.lasso[2,1]
vp <- tab.final.lasso[2,2]

error.prueba.lasso <- (fp + fn ) / length(y.test)
kappa.lasso <- (tasa.base - error.prueba.lasso)/tasa.base

esp.lasso <-vn / (vn + fp)
sen.lasso <- vp / (fn + vp)
pred.lasso <- prediction(preds.1, y.test == 1)
perf.lasso <- performance(pred.lasso, 'sens','fpr')
@

\subsection{Curvas ROC}
<<12.7>>=
plot(perf.ridge, col='green', type='l', xlab = "Falsos Positivos", ylab = "Sensitividad")
plot(perf.lasso, col='purple', add = T, type="l")
abline(a=0, b=1)
@

\section{Ejercicio 13}
Ejercicio de simulación. Se tiene un modelo $x \sim U(0,1)$,  y $y=|x-1/2|+\epsilon$ con $\epsilon \sim N(0,1)$. Se quiere usar 3 vecinos más cercanos para predecir $y$ en función de $x$.\\

Primero se producen 100 muestras de entrenamiento de tamaño n = 30 y una sola muestra de prueba de tamaño grande, en este caso 1000.

<<3.1, message=FALSE>>=
rm(list=ls())

xp <- runif(1000)
yp <- abs(xp-0.5) + rnorm(length(xp))
datos.prueba <- data.frame(x = xp, y = yp)
@
    
Haciendo el ajuste por 3 vecinos más cercanos se obtienen los errores de entrenamiento y de predicción (condicional a cada muestra).
  
<<3.2>>=
salida.sim.l <- lapply(1:100, function(i){
    x <- runif(30)
    y <- abs(x-0.5) + rnorm(length(x))
    datos <- data.frame(x = x,y = y)
    f.hat <- kknn(y ~ x, 
                  train = datos, 
                  test = datos,
                  k = 3, kernel = 'rectangular')
    error.entrena <- mean((fitted(f.hat) - datos$y)^2)
  

    f.hat2 <- kknn(y ~ x,
                  train = datos,
                  test = datos.prueba,
                  k = 3, kernel = 'rectangular')
    error.prueba <- mean((fitted(f.hat2)-datos.prueba$y)^2)     
    
    data.frame(muestra = i,error.entrena, error.prueba)
})

  salida.sim <- rbind_all(salida.sim.l)
  salida.sim.m <- melt(salida.sim, id.vars='muestra')

@

A continuación se muestra una gráfica con los errores de predicción y de entrenamiento para cada muestar de entrenamiento. 
<<3.3>>=
  ggplot(salida.sim.m,aes(x = muestra, y = value,col = variable, group = variable)) +
         geom_line() + geom_point() + theme_bw() +
         theme(legend.position="top", legend.title=element_blank(),
               axis.title.y = element_blank()) + xlab("Numero de muestra") +
         scale_color_discrete(labels = c("Error de Entrenamiento", "Error de Prueba"))
@
Ahora, podemos usar las 100 muestras para estimar el error de predicción no condicional, que es igual al valor esperado sobre la muestra de los errores de predicción condicional y que se aproxima con el promedio de los errores de predicción.
<<3.4, message=FALSE>>=
  ggplot(salida.sim, aes(x=error.prueba)) + theme_bw() +
    geom_histogram(position = "dodge") +
    geom_vline(data = data.frame(val = mean(salida.sim$error.prueba)), 
               aes(xintercept = val, color = "red"), show_guide = TRUE) +
    xlab("Error de prueba condicional") + ylab("Conteo") +
    theme(legend.position="top", legend.title=element_blank()) +
    scale_color_discrete(labels = c("Error de prueba no condicional"))
@
  \subsection{Resultados}
  \begin{itemize}
    \item A partir de las gráficas se puede observar que el error de entrenamiento suele ser menor que el error de prueba. \\
    \item Existe variabilidad en el error de prueba condicional.
  \end{itemize}
\end{document}